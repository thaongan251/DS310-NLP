{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install numpy==1.17.4\n!pip install nltk==3.4.5\n!pip install torchtext==0.4.0\n!pip install scikit_learn==0.23.2\n!pip install spacy==2.3.5\n!pip install textblob==0.15.3\n!pip install torch==1.6.0 \n!pip install torchvision==0.7.0\n!pip install tqdm\n!pip install underthesea","metadata":{"id":"Plix5R5piR8g","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\n\nimport os\nimport math\nimport random\nimport argparse\nfrom pathlib import Path\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchtext.data import BucketIterator\n\nfrom torchtext.data import Field, Example, Dataset\nimport re\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport random\nimport numpy as np\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm_notebook\nimport time\nfrom torchtext.data import BucketIterator\nimport gc\n\npath = '/kaggle/working/' #colab: /content/ or bla bla...\nimport sys\nsys.argv=['']\ndel sys","metadata":{"id":"hbzYULcCkwmJ","outputId":"0dd025c7-f1ee-4352-80ff-34dc4aa2140e","execution":{"iopub.status.busy":"2021-11-30T03:09:13.717133Z","iopub.execute_input":"2021-11-30T03:09:13.717539Z","iopub.status.idle":"2021-11-30T03:09:14.742255Z","shell.execute_reply.started":"2021-11-30T03:09:13.717416Z","shell.execute_reply":"2021-11-30T03:09:14.741486Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def parse_args():\n    \"\"\"Add arguments to parser\"\"\"\n    parser = argparse.ArgumentParser(description='Verbalization dataset baseline models.')\n    parser.add_argument('--model', default=RNN_NAME, type=str,\n                        choices=[RNN_NAME], help='model to train the dataset')\n    parser.add_argument('--input', default=QUESTION, type=str,\n                        choices=[QUESTION], help='use question as input')\n    parser.add_argument('--attention', default=ATTENTION_2, type=str,\n                        choices=[ATTENTION_2], help='attention layer for rnn model')\n    args = parser.parse_args()\n    return args\n\ndef set_SEED():\n    SEED = 42\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.enabled = False\n    torch.backends.cudnn.deterministic = True","metadata":{"id":"m8lTXOWvkQoj","execution":{"iopub.status.busy":"2021-11-30T03:09:49.863862Z","iopub.execute_input":"2021-11-30T03:09:49.864136Z","iopub.status.idle":"2021-11-30T03:09:49.872583Z","shell.execute_reply.started":"2021-11-30T03:09:49.864106Z","shell.execute_reply":"2021-11-30T03:09:49.871090Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\"\"\"Constants for the baseline models\"\"\"\nSEED = 42\nQUESTION = 'question'\n\nRNN_NAME = 'rnn'\nCNN_NAME = 'cnn'\nTRANSFORMER_NAME = 'transformer'\n\nATTENTION_1 = 'bahdanau'\nATTENTION_2 = 'luong'\n\nGPU = 'gpu'\nCPU = 'cpu'\nCUDA = 'cuda'\n\nCHECKPOINT_PATH = '/model/'\n\nANSWER_TOKEN = '<ans>'\nENTITY_TOKEN = '<ent>'\nEOS_TOKEN = '<eos>'\nSOS_TOKEN = '<sos>'\nPAD_TOKEN = '<pad>'\n\nSRC_NAME = 'src'\nTRG_NAME = 'trg'","metadata":{"id":"XK48q1VNj_3g","execution":{"iopub.status.busy":"2021-11-30T03:09:54.955759Z","iopub.execute_input":"2021-11-30T03:09:54.956420Z","iopub.status.idle":"2021-11-30T03:09:54.962326Z","shell.execute_reply.started":"2021-11-30T03:09:54.956374Z","shell.execute_reply":"2021-11-30T03:09:54.961592Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Chechpoint(object):\n    \"\"\"Checkpoint class\"\"\"\n    @staticmethod\n    def save(model, path):\n        \"\"\"Save model using name\"\"\"\n        name = f'{model.name}.pt'\n        torch.save(model.state_dict(), path+name)\n\n    @staticmethod\n    def load(model,path, name):\n        \"\"\"Load model using name\"\"\"\n        #name = f'{model.name}.pt'\n        model.load_state_dict(torch.load(path+name))\n        return model","metadata":{"execution":{"iopub.status.busy":"2021-11-30T03:09:59.207751Z","iopub.execute_input":"2021-11-30T03:09:59.208273Z","iopub.status.idle":"2021-11-30T03:09:59.213635Z","shell.execute_reply.started":"2021-11-30T03:09:59.208233Z","shell.execute_reply":"2021-11-30T03:09:59.212914Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from underthesea import word_tokenize\n\nclass VerbalDataset(object):\n    \"\"\"VerbalDataset class\"\"\"\n#     TOKENIZE_SEQ = lambda self, x: x.replace(\"?\", \" ?\").\\\n#                                      replace(\".\", \" .\").\\\n#                                      replace(\",\", \" ,\").\\\n#                                      replace(\"'\", \" '\").\\\n#                                      split()\n                                         \n    def __init__(self,train,test):\n        self.train = train\n        self.test = test\n        self.train_data = None\n        self.valid_data = None\n        self.test_data = None\n        self.src_field = None\n        self.trg_field = None\n\n    def _extract_question_answer2(self, train, test):\n        return [[data['question'], [data['verbalized_answer'], data['verbalized_answer_2'],data['verbalized_answer_3'],data['verbalized_answer_4'],data['verbalized_answer_5'],data['verbalized_answer_6'],data['verbalized_answer_7'],data['verbalized_answer_8']]] for data in train], \\\n                [[data['question'], [data['verbalized_answer'], data['verbalized_answer_2'],data['verbalized_answer_3'],data['verbalized_answer_4'],data['verbalized_answer_5'],data['verbalized_answer_6'],data['verbalized_answer_7'],data['verbalized_answer_8']]] for data in test]\n\n    def _extract_question_answer(self, train, test):\n        return [[data['Question'], data['Answer']] for data in train], \\\n                [[data['Question'], data['Answer']] for data in test]\n\n\n    def _make_torchtext_dataset(self, data, fields):\n        examples = [Example.fromlist(i, fields) for i in data]\n        return Dataset(examples, fields)\n\n    def load_data_and_fields(self, ):\n        \"\"\"\n        Load verbalization data\n        Create source and target fields\n        \"\"\"\n        train, test, val = [], [], []\n        \n        train = self.train\n        test = self.test\n\n        # split test data to val-test\n        test, val = train_test_split(test, test_size=0.5, shuffle=False)\n\n        # create fields\n        self.src_field = Field(tokenize=word_tokenize,\n                               init_token=SOS_TOKEN,\n                               eos_token=EOS_TOKEN,\n                               lower=True,\n                               include_lengths=True,\n                               batch_first=True)\n        \n        self.trg_field = Field(tokenize=word_tokenize,\n                               init_token=SOS_TOKEN,\n                               eos_token=EOS_TOKEN,\n                               lower=True,\n                               batch_first=True)\n\n        fields_tuple = [(SRC_NAME, self.src_field), (TRG_NAME, self.trg_field)]\n\n        # create toechtext datasets\n        self.train_data = self._make_torchtext_dataset(train, fields_tuple)\n        self.valid_data = self._make_torchtext_dataset(val, fields_tuple)\n        self.test_data = self._make_torchtext_dataset(test, fields_tuple)\n\n        # build vocabularies\n        self.src_field.build_vocab(self.train_data, min_freq=2)\n        self.trg_field.build_vocab(self.train_data, min_freq=2)\n        print(\"i am field tuple\",fields_tuple)\n\n    def get_data(self):\n        \"\"\"Return train, validation and test data objects\"\"\"\n        return self.train_data, self.valid_data, self.test_data\n\n    def get_fields(self):\n        \"\"\"Return source and target field objects\"\"\"\n        return self.src_field, self.trg_field\n\n    def get_vocabs(self):\n        \"\"\"Return source and target vocabularies\"\"\"\n        #print('self, trg field vocab: ', self.trg_field.vocab)\n        return self.src_field.vocab, self.trg_field.vocab","metadata":{"id":"2C-14ZkGhf8O","execution":{"iopub.status.busy":"2021-11-30T03:10:02.902660Z","iopub.execute_input":"2021-11-30T03:10:02.903133Z","iopub.status.idle":"2021-11-30T03:10:04.935725Z","shell.execute_reply.started":"2021-11-30T03:10:02.903097Z","shell.execute_reply":"2021-11-30T03:10:04.934965Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"set_SEED()\nargs = parse_args()\n\ndf = pd.read_csv('/kaggle/input/covid19-qa/Covid19_QA(4300).csv')\ndf = df[['Question','Answer']]\ntrain, test = train_test_split(df.values,train_size=0.814,random_state=42)\n\ndataset = VerbalDataset(train,test)\n\ndataset.load_data_and_fields()\nsrc_vocab, trg_vocab = dataset.get_vocabs()\ntrain_data, valid_data, test_data = dataset.get_data()","metadata":{"execution":{"iopub.status.busy":"2021-11-30T03:10:10.078054Z","iopub.execute_input":"2021-11-30T03:10:10.078820Z","iopub.status.idle":"2021-11-30T03:11:40.293736Z","shell.execute_reply.started":"2021-11-30T03:10:10.078778Z","shell.execute_reply":"2021-11-30T03:11:40.292931Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print('--------------------------------')\nprint(f\"Training data: {len(train_data.examples)}\")\nprint(f\"Evaluation data: {len(valid_data.examples)}\")\nprint(f\"Testing data: {len(test_data.examples)}\")\nprint('--------------------------------')\nprint(f'Question example: {train_data.examples[2].src}\\n')\nprint(f'Answer example: {train_data.examples[2].trg}')\nprint('--------------------------------')\nprint(f\"Unique tokens in questions vocabulary: {len(src_vocab)}\")\nprint(f\"Unique tokens in answers vocabulary: {len(trg_vocab)}\")\nprint('--------------------------------')","metadata":{"execution":{"iopub.status.busy":"2021-11-30T03:13:49.274794Z","iopub.execute_input":"2021-11-30T03:13:49.275066Z","iopub.status.idle":"2021-11-30T03:13:49.283365Z","shell.execute_reply.started":"2021-11-30T03:13:49.275038Z","shell.execute_reply":"2021-11-30T03:13:49.282620Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Sequence-to-Sequence Model","metadata":{}},{"cell_type":"markdown","source":"### Layers","metadata":{}},{"cell_type":"code","source":"def RNN(cell_name):\n    if cell_name.lower() == 'lstm':\n        return LSTM\n    elif cell_name.lower() == 'gru':\n        return GRU\n    else:\n        raise ValueError(f\"Unsupported RNN Cell: {cell_name}\")\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx):\n    \"\"\"Embedding layer\"\"\"\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    nn.init.uniform_(m.weight, -0.1, 0.1)\n    nn.init.constant_(m.weight[padding_idx], 0)\n    return m\n\ndef Linear(in_features, out_features, bias=True):\n    \"\"\"Linear layer\"\"\"\n    m = nn.Linear(in_features, out_features, bias=bias)\n    m.weight.data.uniform_(-0.1, 0.1)\n    if bias:\n        m.bias.data.uniform_(-0.1, 0.1)\n    return m\n\ndef LSTM(input_size, hidden_size, **kwargs):\n    \"\"\"LSTM layer\"\"\"\n    m = nn.LSTM(input_size, hidden_size, **kwargs)\n    for name, param in m.named_parameters():\n        if 'weight' in name or 'bias' in name:\n            param.data.uniform_(-0.1, 0.1)\n    return m\n\ndef GRU(input_size, hidden_size, **kwargs):\n    \"\"\"GRU layer\"\"\"\n    m = nn.GRU(input_size, hidden_size, **kwargs)\n    for name, param in m.named_parameters():\n        if 'weight' in name or 'bias' in name:\n            param.data.uniform_(-0.1, 0.1)\n    return m","metadata":{"execution":{"iopub.status.busy":"2021-11-30T03:33:41.757793Z","iopub.execute_input":"2021-11-30T03:33:41.758260Z","iopub.status.idle":"2021-11-30T03:33:41.768855Z","shell.execute_reply.started":"2021-11-30T03:33:41.758216Z","shell.execute_reply":"2021-11-30T03:33:41.768158Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Encoder","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \"\"\"Encoder\"\"\"\n    def __init__(self, vocabulary, device, hidden_size=512, num_layers=2,\n                 bidirectional=True, cell_name='gru', dropout=0.5):\n        super().__init__()\n        input_dim = len(vocabulary)\n        self.num_layers = num_layers\n        self.pad_id = vocabulary.stoi[PAD_TOKEN]\n        self.hidden_size = hidden_size\n        self.bidirectional = bidirectional\n        self.device = device\n        self.rnn_cell = RNN(cell_name)\n\n        self.embedding = Embedding(input_dim, self.hidden_size, self.pad_id)\n        self.dropout = dropout\n\n        self.rnn = self.rnn_cell(\n            input_size=self.hidden_size,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            batch_first=True,\n            bidirectional=self.bidirectional,\n            dropout=self.dropout if self.num_layers > 1 else 0.\n        )\n\n    def forward(self, src_tokens, **kwargs):\n        \"\"\"\n        Forward Encoder\n        Args:\n            src_tokens (LongTensor): (batch, src_len)\n            src_lengths (LongTensor): (batch)\n        Returns:\n            x (LongTensor): (src_len, batch, hidden_size * num_directions)\n            hidden (LongTensor): (batch, enc_hid_dim)\n        \"\"\"\n        src_lengths = kwargs.get('src_lengths', '')\n\n        embedded = self.embedding(src_tokens)\n        embedded = F.dropout(embedded, p=self.dropout, training=self.training)\n\n        embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_lengths, batch_first=True)\n        output, hidden = self.rnn(embedded)\n\n        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n        output = F.dropout(output, p=self.dropout, training=self.training)\n\n        if isinstance(hidden, tuple):\n            hidden = tuple([self._cat_directions(h) for h in hidden])\n        else:\n            hidden = self._cat_directions(hidden)\n\n        return output, hidden\n\n    def _cat_directions(self, h):\n        \"\"\"\n        If the encoder is bidirectional, do the following transformation.\n        (#directions * #layers, #batch, hidden_size) -> (#layers, #batch, #directions * hidden_size)\n        \"\"\"\n        if self.bidirectional:\n            h = torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\n        return h\n","metadata":{"id":"OLsf7a0jmLOO","execution":{"iopub.status.busy":"2021-11-30T03:33:45.020841Z","iopub.execute_input":"2021-11-30T03:33:45.021502Z","iopub.status.idle":"2021-11-30T03:33:45.034349Z","shell.execute_reply.started":"2021-11-30T03:33:45.021447Z","shell.execute_reply":"2021-11-30T03:33:45.033494Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Attention","metadata":{}},{"cell_type":"code","source":"class Attention(nn.Module):\n    \"\"\"Attention\"\"\"\n    def __init__(self, input_embed, source_embed, output_embed):\n        super().__init__()\n        self.linear_in = Linear(input_embed, source_embed)\n        self.linear_out = Linear(input_embed+source_embed, output_embed)\n\n    def forward(self, output, context, mask):\n        \"\"\"\n        Forward Attention\n        \"\"\"\n        # input: bsz x input_embed_dim\n        # source_hids: srclen x bsz x source_embed_dim\n\n        input = output.squeeze(1)\n        source_hids = context.permute(1, 0, 2)\n\n        x = self.linear_in(input)\n\n        # compute attention\n        attn_scores = (source_hids * x.unsqueeze(0)).sum(dim=2)\n\n        # don't attend over padding\n        attn_scores = attn_scores.float().masked_fill(mask == 0, float('-inf'))\n\n        attn_scores = F.softmax(attn_scores, dim=0)  # srclen x bsz\n\n        # sum weighted sources\n        x = (attn_scores.unsqueeze(2) * source_hids).sum(dim=0)\n\n        x = torch.cat((x, input), dim=1)\n        x = self.linear_out(x)\n        x = torch.tanh(x)\n\n        return x, attn_scores\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decoder","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    \"\"\"Decoder\"\"\"\n    def __init__(self, vocabulary, device, hidden_size=512, num_layers=2,\n                 max_len=50, cell_name='gru', dropout=0.5):\n        super().__init__()\n        self.output_dim = len(vocabulary)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.max_length = max_len\n        self.device = device\n        self.eos_id = vocabulary.stoi[EOS_TOKEN]\n        self.sos_id = vocabulary.stoi[SOS_TOKEN]\n        self.pad_id = vocabulary.stoi[PAD_TOKEN]\n        self.rnn_cell = RNN(cell_name)\n\n        self.encoder_proj = Linear(hidden_size*2, hidden_size)\n\n        self.embedding = Embedding(self.output_dim, self.hidden_size, self.pad_id)\n        self.dropout = dropout\n\n        self.rnn = self.rnn_cell(\n            input_size=hidden_size,\n            hidden_size=hidden_size,\n            num_layers=self.num_layers,\n            batch_first=True,\n            dropout=self.dropout if num_layers > 1 else 0.\n        )\n\n        self.attention = Attention(self.hidden_size, self.hidden_size*2, self.hidden_size)\n        self.linear_out = Linear(self.hidden_size, self.output_dim)\n\n    def _decoder_step(self, input_var, hidden, encoder_outputs, mask):\n        input_var = input_var.unsqueeze(1)\n\n        embedded = self.embedding(input_var)\n        embedded = F.dropout(embedded, p=self.dropout, training=self.training)\n\n        output, hidden = self.rnn(embedded, hidden)\n        output = F.dropout(output, p=self.dropout, training=self.training)\n\n        output, attn = self.attention(output, encoder_outputs, mask)\n        output = F.dropout(output, p=self.dropout, training=self.training)\n\n        output = self.linear_out(output)\n        # output = F.dropout(output, p=self.dropout, training=self.training)\n        output = F.log_softmax(output, dim=1)\n\n        return output, hidden, attn\n\n    def forward(self, trg_tokens, encoder_out, **kwargs):\n        \"\"\"\n        Forward Decoder\n        \"\"\"\n        encoder_out, hidden = encoder_out\n        src_tokens = kwargs.get('src_tokens', '')\n        teacher_forcing_ratio = kwargs.get('teacher_forcing_ratio', '')\n        batch_size, src_length = src_tokens.size()\n\n        if trg_tokens is None:\n            teacher_forcing_ratio = 0.\n            inference = True\n            trg_tokens = torch.zeros((batch_size, self.max_length)).long().\\\n                                                                      fill_(self.sos_id).\\\n                                                                      to(self.device)\n        else:\n            inference = False\n\n        max_length = trg_tokens.shape[1]\n\n        outputs = torch.zeros(max_length, batch_size, self.output_dim).to(self.device)\n        attentions = torch.zeros(max_length, batch_size, src_length).to(self.device)\n\n        mask = (src_tokens != self.pad_id).t()\n\n        # check whether encoder has lstm or gru hidden state and\n        # project their output to decoder hidden state\n        if isinstance(hidden, tuple):\n            hidden = [self.encoder_proj(h) for h in hidden] # new_line\n        else:\n            hidden = self.encoder_proj(hidden)\n\n        # use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n        decoder_input = trg_tokens[:, 0]\n\n        # Here we miss the output for position 0\n        for i in range(1, max_length):\n            output, hidden, attention = self._decoder_step(decoder_input, hidden, encoder_out, mask)\n            outputs[i] = output\n            attentions[i] = attention.t()\n            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n            decoder_input = trg_tokens[:, i] if use_teacher_forcing else output.argmax(1)\n\n            if inference and decoder_input.item() == self.eos_id and i > 0:\n                return outputs[:i] # , attentions[:i]\n\n        return outputs # , attentions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Seq2Seq","metadata":{}},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, name):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.name = name\n\n    def forward(self, src_tokens, src_lengths, trg_tokens, teacher_forcing_ratio=0.5):\n        encoder_out = self.encoder(src_tokens, \n                                   src_lengths=src_lengths)\n        \n        decoder_out = self.decoder(trg_tokens, encoder_out,\n                                   src_tokens=src_tokens,\n                                   teacher_forcing_ratio=teacher_forcing_ratio)\n        return decoder_out","metadata":{"id":"mTH7W-LAnU-n","execution":{"iopub.status.busy":"2021-11-30T03:33:54.407549Z","iopub.execute_input":"2021-11-30T03:33:54.408088Z","iopub.status.idle":"2021-11-30T03:33:54.414100Z","shell.execute_reply.started":"2021-11-30T03:33:54.408049Z","shell.execute_reply":"2021-11-30T03:33:54.412975Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device(CUDA if torch.cuda.is_available() else CPU)\nencoder = Encoder(src_vocab, DEVICE)\ndecoder = Decoder(trg_vocab, DEVICE)\nmodel = Seq2Seq(encoder, decoder, args.model).to(DEVICE)\n\nparameters_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint('--------------------------------')\nprint(f'Model: {args.model}')\nprint(f'Model input: {args.input}')\nif args.model == RNN_NAME:\n    print(f'Attention: {args.attention}')\nprint(f'The model has {parameters_num:,} trainable parameters')\nprint('--------------------------------')","metadata":{"id":"MuysOCKGnH5E","outputId":"c64aa408-b6e8-447a-81e3-f7bb7d66c0e7","execution":{"iopub.status.busy":"2021-11-30T03:14:45.817143Z","iopub.execute_input":"2021-11-30T03:14:45.817414Z","iopub.status.idle":"2021-11-30T03:14:48.193984Z","shell.execute_reply.started":"2021-11-30T03:14:45.817383Z","shell.execute_reply":"2021-11-30T03:14:48.193241Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"markdown","source":"### Evaluator","metadata":{}},{"cell_type":"code","source":"class Evaluator(object):\n    \"\"\"Evaluator class\"\"\"\n    def __init__(self, criterion):\n        self.criterion = criterion\n\n    def evaluate(self, model, iterator, teacher_ratio=1.0):\n        model.eval()\n        epoch_loss = 0\n        with torch.no_grad():\n            for _, batch in enumerate(iterator):\n                src, src_len = batch.src\n                trg = batch.trg\n                input_trg = trg if model.name == RNN_NAME else trg[:, :-1]\n                output = model(src, src_len, input_trg, teacher_ratio)\n                trg = trg.t() if model.name == RNN_NAME else trg[:, 1:]\n                output = output.contiguous().view(-1, output.shape[-1])\n                trg = trg.contiguous().view(-1)\n                # output: (batch_size * trg_len) x output_dim\n                # trg: (batch_size * trg_len)\n                loss = self.criterion(output, trg)\n                epoch_loss += loss.item()\n        return epoch_loss / len(iterator)","metadata":{"id":"6HYt7AsYn9pu","execution":{"iopub.status.busy":"2021-11-30T03:33:58.038556Z","iopub.execute_input":"2021-11-30T03:33:58.039202Z","iopub.status.idle":"2021-11-30T03:33:58.048894Z","shell.execute_reply.started":"2021-11-30T03:33:58.039163Z","shell.execute_reply":"2021-11-30T03:33:58.047718Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Trainer","metadata":{}},{"cell_type":"code","source":"class Trainer(object):\n    \"\"\"Trainer Class\"\"\"\n    def __init__(self, optimizer, criterion, batch_size, device):\n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.batch_size = batch_size\n        self.device = device\n        self.evaluator = Evaluator(criterion=self.criterion)\n\n    def _train_batch(self, model, iterator, teacher_ratio, clip):\n        model.train()\n        epoch_loss = 0\n        for _, batch in enumerate(tqdm_notebook(iterator)):\n            src, src_len = batch.src\n            trg = batch.trg\n            self.optimizer.zero_grad()\n            input_trg = trg if model.name == RNN_NAME else trg[:, :-1]\n            output = model(src, src_len, input_trg, teacher_ratio)\n            trg = trg.t() if model.name == RNN_NAME else trg[:, 1:]\n            output = output.contiguous().view(-1, output.shape[-1])\n            trg = trg.contiguous().view(-1)\n            # output: (batch_size * trg_len) x output_dim\n            # trg: (batch_size * trg_len)\n            torch.cuda.empty_cache()\n            loss = self.criterion(output, trg)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n            self.optimizer.step()\n            epoch_loss += loss.item()\n        return epoch_loss / len(iterator)\n\n    def _get_iterators(self, train_data, valid_data, model_name):\n        return BucketIterator.splits((train_data, valid_data),\n                                     batch_size=self.batch_size,\n                                     sort_within_batch=True if model_name == RNN_NAME else \\\n                                                       False,\n                                     sort_key=lambda x: len(x.src),\n                                     device=self.device)\n\n    def _epoch_time(self, start_time, end_time):\n        elapsed_time = end_time - start_time\n        elapsed_mins = int(elapsed_time / 60)\n        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n        return elapsed_mins, elapsed_secs\n\n    def _log_epoch(self, train_loss, valid_loss, epoch, start_time, end_time):\n        minutes, seconds = self._epoch_time(start_time, end_time)\n        print(f'Epoch: {epoch+1:02} | Time: {minutes}m {seconds}s')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):7.3f}')\n\n    def _train_epoches(self, model, train_data, valid_data,  path_, num_of_epochs, teacher_ratio, clip):\n        best_valid_loss = float('inf')\n        # pylint: disable=unbalanced-tuple-unpacking\n        train_iterator, valid_iterator = self._get_iterators(train_data, valid_data, model.name)\n        for epoch in range(num_of_epochs):\n            start_time = time.time()\n            train_loss = self._train_batch(model, train_iterator, teacher_ratio, clip)\n            valid_loss = self.evaluator.evaluate(model, valid_iterator, teacher_ratio)\n            end_time = time.time()\n            self._log_epoch(train_loss, valid_loss, epoch, start_time, end_time)\n            if valid_loss < best_valid_loss:\n                best_valid_loss = valid_loss\n                Chechpoint.save(model,path_)\n\n    def train(self, model, train_data, valid_data, path_, num_of_epochs=20, teacher_ratio=1.0, clip=1):\n        \"\"\"Train model\"\"\"\n        self._train_epoches(model, train_data, valid_data, path_, num_of_epochs, teacher_ratio, clip)","metadata":{"id":"i5F6Cqfvn3cY","execution":{"iopub.status.busy":"2021-11-30T03:34:00.370653Z","iopub.execute_input":"2021-11-30T03:34:00.371074Z","iopub.status.idle":"2021-11-30T03:34:00.393287Z","shell.execute_reply.started":"2021-11-30T03:34:00.371030Z","shell.execute_reply":"2021-11-30T03:34:00.390067Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Train model","metadata":{}},{"cell_type":"code","source":"# create optimizer\noptimizer = optim.Adam(model.parameters(),lr=0.001)\n# define criterion\ncriterion = nn.CrossEntropyLoss(ignore_index=trg_vocab.stoi[PAD_TOKEN])\n# batch_size\nbatch_size = 12\n\ntrainer = Trainer(optimizer, criterion, batch_size, DEVICE)\ntrainer.train(model, train_data, valid_data, path, num_of_epochs=1)","metadata":{"id":"ANwkMQcXoBSk","execution":{"iopub.status.busy":"2021-11-30T03:34:22.123726Z","iopub.execute_input":"2021-11-30T03:34:22.124188Z","iopub.status.idle":"2021-11-30T03:39:19.319602Z","shell.execute_reply.started":"2021-11-30T03:34:22.124148Z","shell.execute_reply":"2021-11-30T03:39:19.318818Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Predict and Evaluation","metadata":{}},{"cell_type":"markdown","source":"### BleuScorer (gồm BLEU và METEOR)","metadata":{}},{"cell_type":"code","source":"import nltk\n\nclass BleuScorer(object):\n    \"\"\"Blue scorer class\"\"\"\n    def __init__(self):\n        self.results = []\n        self.results_meteor = []\n        self.score = 0\n        self.meteor_score = 0\n        self.instances = 0\n        self.meteor_instances = 0\n\n    def example_score(self, reference, hypothesis):\n        \"\"\"Calculate blue score for one example\"\"\"\n        return nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,weights=(1,0,0,0)) #unigram\n    \n    def example_score_meteor(self, reference, hypothesis):\n        \"\"\"Calculate blue score for one example\"\"\"\n        return nltk.translate.meteor_score.single_meteor_score(reference,hypothesis)\n\n    def data_score(self, data, predictor, path):\n        \"\"\"Score complete list of data\"\"\"\n        results_prelim = []\n        for example in data:\n            #i = 1\n            src = [t.lower() for t in example.src]\n            reference = [t.lower() for t in example.trg]\n            # loop through example.src and calculate all hypothesis(max. 8) \n            #and calculate blue score average of all hypothesis\n            hypothesis = predictor.predict(example.src)\n            blue_score = self.example_score(reference, hypothesis)\n            meteor_score = self.example_score_meteor(' '.join(reference), ' '.join(hypothesis))\n            #print('Blue Score: ',blue_score)\n            results_prelim.append({\n                'question': '\"' + str(src) + '\"',\n                'reference': reference,\n                'hypothesis': hypothesis,\n                'blue_score': blue_score,\n                'meteor_score': meteor_score\n            })\n        #print('List length before aggregation',len(results_prelim))\n\n        results = [max((v for v in results_prelim if v['question'] == x), key=lambda y:y['blue_score']) for x in set(v['question'] for v in results_prelim)] \n\n        with open(path+'result_output.txt', 'w') as f:\n            for elem in results:\n                f.write(\"%s\\n\" % elem)\n                self.results.append(elem)\n                self.score += elem['blue_score']\n                self.meteor_score += elem['meteor_score']\n                self.instances += 1\n        return self.score / self.instances, self.meteor_score / self.instances\n\n    def average_score(self):\n        \"\"\"Return bleu average score\"\"\"\n        return self.score / self.instances\n    \n    def data_meteor_score(self, data, predictor, path):\n        \"\"\"Score complete list of data\"\"\"\n        results_prelim = []\n        for example in data:\n            src = [t.lower() for t in example.src]\n            reference = [t.lower() for t in example.trg]\n            hypothesis = predictor.predict(example.src)\n            meteor_score = self.example_score_meteor(' '.join(reference), ' '.join(hypothesis))\n            results_prelim.append({\n                'question': '\"' + str(src) + '\"',\n                'reference': reference,\n                'hypothesis': hypothesis,\n                'meteor_score': meteor_score\n            })\n        results_meteor = [max((v for v in results_prelim if v['question'] == x), key=lambda y:y['meteor_score']) for x in set(v['question'] for v in results_prelim)] \n\n        with open(path+'result_meteor_output.txt', 'w') as f:\n            for elem in results_meteor:\n                f.write(\"%s\\n\" % elem)\n                self.results_meteor.append(elem)\n                self.meteor_score += elem['meteor_score']\n                self.meteor_instances += 1\n        return self.meteor_score/self.meteor_instances\n    \n    def average_meteor_score(self):\n        \"\"\"Return meteor average score\"\"\"\n        return self.meteor_score/self.instances\n\n    def reset(self):\n        \"\"\"Reset object properties\"\"\"\n        self.results = []\n        self.results_meteor = []\n        self.score = 0\n        self.meteor_score = 0\n        self.instances = 0\n        self.meteor_instances = 0","metadata":{"execution":{"iopub.status.busy":"2021-11-30T03:39:19.322113Z","iopub.execute_input":"2021-11-30T03:39:19.322397Z","iopub.status.idle":"2021-11-30T03:39:19.342721Z","shell.execute_reply.started":"2021-11-30T03:39:19.322360Z","shell.execute_reply":"2021-11-30T03:39:19.341914Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Predictor","metadata":{}},{"cell_type":"code","source":"class Predictor(object):\n    \"\"\"Predictor class\"\"\"\n    def __init__(self, model, src_vocab, trg_vocab, device):\n        self.model = model\n        self.src_vocab = src_vocab\n        self.trg_vocab = trg_vocab\n        self.device = device\n\n    def _predict_step(self, tokens):\n        self.model.eval()\n        tokenized_sentence = [SOS_TOKEN] + [t.lower() for t in tokens] + [EOS_TOKEN]\n        numericalized = [self.src_vocab.stoi[token] for token in tokenized_sentence]\n        src_tensor = torch.LongTensor(numericalized).unsqueeze(0).to(self.device)\n\n        with torch.no_grad():\n            encoder_out = self.model.encoder(src_tensor)\n\n        outputs = [self.trg_vocab.stoi[SOS_TOKEN]]\n\n        # cnn positional embedding gives assertion error for tensor\n        # of size > max_positions-1, we predict tokens for max_positions-2\n        # to avoid the error\n        for _ in range(self.model.decoder.max_positions-2):\n            trg_tensor = torch.LongTensor(outputs).unsqueeze(0).to(self.device)\n\n            with torch.no_grad():\n                output = self.model.decoder(trg_tensor, encoder_out, src_tokens=src_tensor)\n\n            prediction = output.argmax(2)[:, -1].item()\n\n            if prediction == self.trg_vocab.stoi[EOS_TOKEN]:\n                break\n\n            outputs.append(prediction)\n\n        translation = [self.trg_vocab.itos[i] for i in outputs]\n\n        return translation[1:] # , attention\n\n    def _predict_rnn_step(self, tokens):\n        self.model.eval()\n        with torch.no_grad():\n            tokenized_sentence = [SOS_TOKEN] + [t.lower() for t in tokens] + [EOS_TOKEN]\n            numericalized = [self.src_vocab.stoi[t] for t in tokenized_sentence]\n\n            src_len = torch.LongTensor([len(numericalized)]).to(self.device)\n            tensor = torch.LongTensor(numericalized).unsqueeze(1).to(self.device)\n\n            translation_tensor_logits = self.model(tensor.t(), src_len, None)\n\n            translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n            translation = [self.trg_vocab.itos[t] for t in translation_tensor]\n\n        return translation[1:] # , attention\n\n    def predict(self, tokens):\n        \"\"\"Perform prediction on given tokens\"\"\"\n        return self._predict_rnn_step(tokens) if self.model.name == RNN_NAME else \\\n                self._predict_step(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T03:39:19.344094Z","iopub.execute_input":"2021-11-30T03:39:19.344355Z","iopub.status.idle":"2021-11-30T03:39:19.361957Z","shell.execute_reply.started":"2021-11-30T03:39:19.344320Z","shell.execute_reply":"2021-11-30T03:39:19.361241Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Predict and Evaluate","metadata":{}},{"cell_type":"code","source":"model = Chechpoint.load(model,path,'./rnn.pt') # chọn path và tên model cho phù hợp\n\nvalid_iterator, test_iterator = BucketIterator.splits(\n                                    (valid_data, test_data),\n                                    batch_size=12,\n                                    sort_within_batch=True, #if args.model == RNN_NAME else False,\n                                    sort_key=lambda x: len(x.src),\n                                    device=DEVICE)\n\n# evaluate model\nvalid_loss = trainer.evaluator.evaluate(model, valid_iterator)\ntest_loss = trainer.evaluator.evaluate(model, test_iterator)\n\n# calculate blue score for valid and test data\npredictor = Predictor(model, src_vocab, trg_vocab, DEVICE)\n\nvalid_scorer = BleuScorer()\ntest_scorer = BleuScorer()\ntrain_scorer = BleuScorer()\n#bleu score\ntrain_scorer.data_score(train_data.examples, predictor,path)\nvalid_scorer.data_score(valid_data.examples, predictor,path)\ntest_scorer.data_score(test_data.examples, predictor,path)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T03:41:20.278602Z","iopub.execute_input":"2021-11-30T03:41:20.279356Z","iopub.status.idle":"2021-11-30T03:44:52.730109Z","shell.execute_reply.started":"2021-11-30T03:41:20.279320Z","shell.execute_reply":"2021-11-30T03:44:52.729466Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"print(f'| Train Data Average BLEU score {train_scorer.average_score()*100} |')\nprint(f'| Train Data Average METEOR score {train_scorer.average_meteor_score()*100} |\\n')\nprint(f'| Val. Loss: {valid_loss:.3f} | Test PPL: {math.exp(valid_loss):7.3f} |')\nprint(f'| Val. Data Average BLEU score {valid_scorer.average_score()*100} |')\nprint(f'| Val. Data Average METEOR score {valid_scorer.average_meteor_score()*100} |\\n')\nprint(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\nprint(f'| Test Data Average BLEU score {test_scorer.average_score()*100} |')\nprint(f'| Test Data Average METEOR score {test_scorer.average_meteor_score()*100} |')","metadata":{"execution":{"iopub.status.busy":"2021-11-30T03:44:52.732854Z","iopub.execute_input":"2021-11-30T03:44:52.733086Z","iopub.status.idle":"2021-11-30T03:44:52.740825Z","shell.execute_reply.started":"2021-11-30T03:44:52.733055Z","shell.execute_reply":"2021-11-30T03:44:52.740099Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"for ex in test_data.examples[:50]:\n    src_tmp = [t.lower() for t in ex.src]\n\n    reference_tmp = [t.lower() for t in ex.trg]\n\n    hypothesis_tmp = predictor.predict(ex.src)\n    print(\" \".join(src_tmp))\n    print(\" \".join(hypothesis_tmp))\n    print(\" \".join(reference_tmp),'\\n')\n    print('______________________________________________________________________________________________________________________________\\n')","metadata":{"execution":{"iopub.status.busy":"2021-11-30T03:45:16.741488Z","iopub.execute_input":"2021-11-30T03:45:16.742170Z","iopub.status.idle":"2021-11-30T03:45:18.624512Z","shell.execute_reply.started":"2021-11-30T03:45:16.742131Z","shell.execute_reply":"2021-11-30T03:45:18.623791Z"},"trusted":true},"execution_count":28,"outputs":[]}]}